{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christine/miniconda3/envs/mt-class/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 12500 12500\n"
     ]
    }
   ],
   "source": [
    "train_path = \"en_kr_data/train/ko2en_training_csv/ko2en_medical_1_training.csv\"\n",
    "val_path = \"en_kr_data/val/ko2en_validation_csv/ko2en_medical_2_validation.csv\" \n",
    "train_df = pd.read_csv(train_path, sep=',')\n",
    "val_test_df = pd.read_csv(val_path, sep=',')\n",
    "val_df, test_df = val_test_df[:len(val_test_df)//2], val_test_df[len(val_test_df)//2:]\n",
    "print(len(train_df), len(val_df), len(test_df))\n",
    "\n",
    "#sort sentences by length\n",
    "#prune sentences over 20 words\n",
    "#train set small\n",
    "#get scores for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kor_Hang\n",
    "\n",
    "name = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "nllb = AutoModelForSeq2SeqLM.from_pretrained(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christine/miniconda3/envs/mt-class/lib/python3.11/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "However, there is a difference of opinion as to whether this trend will continue in the second half.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.src_lang = \"kor_Hang\"\n",
    "inputs = tokenizer(text=\"다만 하반기에도 이같은 기조가 이어질 지에 대해서는 의견이 분분하다.\", return_tensors=\"pt\")\n",
    "translated_tokens = nllb.generate(\n",
    "    **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"eng_Latn\"]\n",
    ")\n",
    "print(tokenizer.decode(translated_tokens[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christine/miniconda3/envs/mt-class/lib/python3.11/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Although the wound is smaller than the surgical procedure and the risk of infection is lower, there is a limit to the complete removal of the placental endometrial components with a needle.\n",
      "Although the risk of infection is lower as the wound is smaller than during the surgical method, but there is a limit to completely removing the thick and sticky endometriosis composition with a needle.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.src_lang = \"kor_Hang\"\n",
    "inputs = tokenizer(text=train_df['한국어'][0], return_tensors=\"pt\")\n",
    "translated_tokens = nllb.generate(\n",
    "    **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"eng_Latn\"]\n",
    ")\n",
    "print(tokenizer.decode(translated_tokens[0], skip_special_tokens=True))\n",
    "print(train_df['영어'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [1:37:58<00:00,  7.52s/it]  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>분야</th>\n",
       "      <th>한국어</th>\n",
       "      <th>영어</th>\n",
       "      <th>한국어_어절수</th>\n",
       "      <th>영어_단어수</th>\n",
       "      <th>길이_분류</th>\n",
       "      <th>난이도</th>\n",
       "      <th>수행기관</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>212501</td>\n",
       "      <td>의료/보건</td>\n",
       "      <td>김시몬 신천지 대변인은 지난 23일 유튜브 등 사회관계망서비스를 통해 \"코로나 19...</td>\n",
       "      <td>Shincheonji spokesman Kim Simon said through s...</td>\n",
       "      <td>24</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>상</td>\n",
       "      <td>에버트란</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>212502</td>\n",
       "      <td>의료/보건</td>\n",
       "      <td>종근당이 종합구충제 '젤콤'으로 온 가족 기생충을 한 번에 잡는다.</td>\n",
       "      <td>Chong Kun Dang catches a family parasite at on...</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>하</td>\n",
       "      <td>에버트란</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>212503</td>\n",
       "      <td>의료/보건</td>\n",
       "      <td>정 본부장은 \"중국에 다녀오신 분들, 주로 의료계나 시설 종사자분들께는 업무 배제 ...</td>\n",
       "      <td>Director Jung said, \"We are making requests fo...</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>하</td>\n",
       "      <td>에버트란</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>212504</td>\n",
       "      <td>의료/보건</td>\n",
       "      <td>법원이 간호사를 상대로 한 의사의 부적절한 발언을 성희롱으로 보고, 해당 의사는 물...</td>\n",
       "      <td>The court considered the doctor's inappropriat...</td>\n",
       "      <td>18</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>중</td>\n",
       "      <td>에버트란</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>212505</td>\n",
       "      <td>의료/보건</td>\n",
       "      <td>피험자들의 연령과 인구통계학적 요소, 건강 관련 행동 등을 모두 고려해도 결과는 달...</td>\n",
       "      <td>The results did not change even if the subject...</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>하</td>\n",
       "      <td>에버트란</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sid     분야                                                한국어  \\\n",
       "0  212501  의료/보건  김시몬 신천지 대변인은 지난 23일 유튜브 등 사회관계망서비스를 통해 \"코로나 19...   \n",
       "1  212502  의료/보건              종근당이 종합구충제 '젤콤'으로 온 가족 기생충을 한 번에 잡는다.   \n",
       "2  212503  의료/보건  정 본부장은 \"중국에 다녀오신 분들, 주로 의료계나 시설 종사자분들께는 업무 배제 ...   \n",
       "3  212504  의료/보건  법원이 간호사를 상대로 한 의사의 부적절한 발언을 성희롱으로 보고, 해당 의사는 물...   \n",
       "4  212505  의료/보건  피험자들의 연령과 인구통계학적 요소, 건강 관련 행동 등을 모두 고려해도 결과는 달...   \n",
       "\n",
       "                                                  영어  한국어_어절수  영어_단어수  길이_분류  \\\n",
       "0  Shincheonji spokesman Kim Simon said through s...       24      42      4   \n",
       "1  Chong Kun Dang catches a family parasite at on...        9      14      1   \n",
       "2  Director Jung said, \"We are making requests fo...       15      23      3   \n",
       "3  The court considered the doctor's inappropriat...       18      28      3   \n",
       "4  The results did not change even if the subject...       13      18      2   \n",
       "\n",
       "  난이도  수행기관  \n",
       "0   상  에버트란  \n",
       "1   하  에버트란  \n",
       "2   하  에버트란  \n",
       "3   중  에버트란  \n",
       "4   하  에버트란  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "test_df_small = test_df#[:1000]\n",
    "#batches = [test_df.iloc[i:i + batch_size] for i in range(0, len(test_df), batch_size)]\n",
    "batches = [test_df_small.iloc[i:i + batch_size] for i in range(0, len(test_df_small), batch_size)]\n",
    "\n",
    "english_translations = []\n",
    "\n",
    "for df_batch in tqdm(batches):\n",
    "    inputs = tokenizer(text=df_batch['한국어'].tolist(), return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    translated_tokens = nllb.generate(\n",
    "        **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"eng_Latn\"]\n",
    "    )\n",
    "    translated_sentences = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    #print(translated_sentences)\n",
    "    english_translations += translated_sentences\n",
    "\n",
    "df_result = pd.concat(batches, ignore_index=True)\n",
    "#print(df_result)\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1867908859829627\n"
     ]
    }
   ],
   "source": [
    "english_translations_split = [t.split() for t in english_translations]\n",
    "\n",
    "english_sentences = test_df['영어'].tolist()#[:1000]\n",
    "english_sentences_split = [[s.split()] for s in english_sentences]\n",
    "\n",
    "print(corpus_bleu(english_sentences_split, english_translations_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nllb_baseline.txt', 'w') as of:\n",
    "    for t in english_translations:\n",
    "        of.write(t+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import Adafactor\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "#nllb.cuda();\n",
    "optimizer = Adafactor(\n",
    "    [p for p in nllb.parameters() if p.requires_grad],\n",
    "    scale_parameter=False,\n",
    "    relative_step=False,\n",
    "    lr=1e-4,\n",
    "    clip_threshold=1.0,\n",
    "    weight_decay=1e-3,\n",
    ")\n",
    "scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = train_df.copy()\n",
    "data_train = data_train.filter(['한국어', '영어'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['또 이와 관련, 양국 관계자들이 모여 공동선언문을 발표할 예정이다.'], ['And regarding this, officials from the two countries will also gather to issue a joint declaration.'], 'kor_Hang', 'en_Latn')\n"
     ]
    }
   ],
   "source": [
    "LANGS = [('영어', 'en_Latn'), ('한국어', 'kor_Hang')]\n",
    "\n",
    "def get_batch_pairs(batch_size, data=data_train[:1000]):\n",
    "    (l1, long1), (l2, long2) = random.sample(LANGS, 2)\n",
    "    x, y = [], []\n",
    "    for _ in range(batch_size):\n",
    "        item = data.iloc[random.randint(0, len(data)-1)]\n",
    "        x.append(item[l1])\n",
    "        y.append(item[l2])\n",
    "    return x, y, long1, long2\n",
    "\n",
    "print(get_batch_pairs(1, data_train[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 \n",
    "max_length = 60 \n",
    "training_steps = 2000 \n",
    "losses = [] \n",
    "MODEL_SAVE_PATH = '/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2000 [00:15<8:32:07, 15.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.1635966300964355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 190/2000 [1:13:34<17:24:52, 34.64s/it]"
     ]
    }
   ],
   "source": [
    "nllb_new = deepcopy(nllb)\n",
    "nllb_new.train()\n",
    "x, y, loss = None, None, None\n",
    "\n",
    "tq = trange(len(losses), training_steps)\n",
    "for i in tq:\n",
    "    x, y, lang1, lang2 = get_batch_pairs(batch_size)\n",
    "    try:\n",
    "        tokenizer.src_lang = lang1\n",
    "        x = tokenizer(x, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(nllb_new.device)\n",
    "        tokenizer.src_lang = lang2\n",
    "        y = tokenizer(y, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(nllb_new.device)\n",
    "        y.input_ids[y.input_ids == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        loss = nllb_new(**x, labels=y.input_ids).loss\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "    except: \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        x, y, loss = None, None, None\n",
    "        print('error', max(len(s) for s in x + y), e)\n",
    "        continue\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(i, np.mean(losses[-1000:]))\n",
    "\n",
    "    if i % 1000 == 0 and i > 0:\n",
    "        nllb_new.save_pretrained(MODEL_SAVE_PATH)\n",
    "        tokenizer.save_pretrained(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
